# MACHINE_LEARNING_MODEL_IMPLIMENTATION_TASK4

COMPANY : CODTECH IT SOLUTONS
NAME : JAYESH KARANKE
INTERN ID : CT04DF2754
DOMIAN : PYTHON PROGRAMMING
DURATION : 4 WEEKS
MENTOR : NEELA SANTOSH KUMAR
DESCRIPTION OF TASK 4 : Internship Task 4 focused on building a machine learning model for spam message classification using Python, with the goal of accurately predicting whether a message is spam or not. This project provided hands-on experience in data preprocessing, natural language processing (NLP), vectorization, model training, and evaluation. It used the well-known "SMS Spam Collection Dataset" loaded from a CSV file named spam.csv, which contains two columns: one for labels (ham or spam) and the other for the actual message text. To prepare the dataset, the column names were renamed to label and message for clarity, and the labels were converted into binary values, mapping ham to 0 and spam to 1 for model compatibility. The data was then split into training and testing subsets using train_test_split from sklearn.model_selection, with 80% of the data allocated for training and 20% for testing, ensuring reproducibility with random_state=42. Next, the messages were transformed from plain text into numerical feature vectors using CountVectorizer from sklearn.feature_extraction.text, which converts the text into a matrix of token counts. The training messages were vectorized using fit_transform() and the test messages using transform() to ensure the same vocabulary was applied. For classification, a Naive Bayes model (MultinomialNB) from sklearn.naive_bayes was chosen due to its effectiveness in text-based problems. The model was trained using the vectorized training data and labels with .fit(), then predictions were made on the test set using .predict(). After generating predictions, model performance was evaluated using accuracy_score and classification_report from sklearn.metrics, which provided metrics such as precision, recall, f1-score, and overall accuracy. The results showed very high accuracy and strong classification performance for both spam and non-spam messages. For example, the classifier achieved approximately 98.38% accuracy, with a near-perfect precision and recall for class 0 (ham) and high scores for class 1 (spam). These outcomes were printed using print() statements and visualized clearly in the Jupyter Notebook output window, as shown in the screenshot TASK4_OUTPUT.png. The code was implemented and executed in a Jupyter Notebook (.ipynb), demonstrating not only the application of ML techniques but also the use of a professional data science environment. To run this task, users were required to install the necessary Python packages using the following commands: pip install pandas scikit-learn. The task showcased the complete pipeline from raw data ingestion to preprocessing, vectorization, model training, prediction, and evaluation. It was a practical example of applying NLP and machine learning to a real-world problem with measurable results. Furthermore, the task introduced the concept of binary classification and the importance of balanced metrics, particularly when dealing with imbalanced classes such as spam detection, where one class might dominate. Students also learned how to interpret a classification report and use metrics to make informed decisions about model improvements. This foundation can be extended to more advanced techniques such as TF-IDF vectorization, using logistic regression or support vector machines, hyperparameter tuning, or even deep learning approaches with neural networks for larger datasets. Task 4 offered insights into the importance of data cleaning, the role of feature engineering in NLP, and the value of reproducibility through random state settings and consistent data splits. It also enforced best practices in script readability, modularity, and stepwise debugging using print outputs. This task was a gateway into machine learning applications for text classification, email filtering, content moderation, and chatbot response tagging. The use of Naive Bayes, a probabilistic model, helped reinforce understanding of conditional probability in real-life applications. As a future improvement, the script could be extended to include visualizations using libraries like matplotlib or seaborn, or even deployed as a web app using Flask or Streamlit. Overall, Task 4 was highly educational and practical, bridging concepts from data science, machine learning, and software engineering into one coherent and impactful project.
OUTPUT : ![IMAGE](https://github.com/user-attachments/assets/176defd8-d334-43c7-9d29-155a56e4c0b7)
